import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

import operator
from typing import List
from langchain.prompts import PromptTemplate
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from TopicSelectionParserClass import TopicSelectionParser
from langchain.output_parsers import PydanticOutputParser
from AgentStateClass import AgentState
from langgraph.graph import StateGraph, END

from langchain_community.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain_community.tools import DuckDuckGoSearchRun
from bs4 import BeautifulSoup
from langchain.agents.agent_types import AgentType
import requests

from IPython.display import Image, display
import os


os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Config the model
model = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# Config the embedding model
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")

# convert text file to spilted docs using recursivecharectertextsplitter
loader = DirectoryLoader("data", glob="./*.txt", loader_cls=TextLoader)
docs = loader.load()

text_spiltter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)
splitted_docs = text_spiltter.split_documents(documents=docs)

# store spiltted doc to chroma db using embedding model
chroma_db = Chroma.from_documents(splitted_docs, embeddings)


# create retriever for chroma db using simularity search
retriever = chroma_db.as_retriever(search_kwargs={"k": 5})


# this pydantic parser used to determine user input topic is related to usa economy or not at supervisor level
pydantic_parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)


# define supervisor to decide user input is topic
def supervisor(state: AgentState):
    # supervisor take latest question from user
    question = state["messages"][-1]

    print("Question is --> ", question)

    template = """
    Classify the user query into one of the categories:
    - "USA": If the query is about the USA economy, government, or policies.
    - "From Internet": If the answer likely depends on up-to-date or real-world information not provided in context (e.g., rankings, live stats, latest news, comparisons).
    - "Not Related": If it is irrelevant to USA or doesn't need internet data.
    
    Only respond with one of the category names above.
    User query: {question}
    {format_instructions}
    """

    prompt = PromptTemplate(
        template=template,
        input_variable=["question"],
        partial_variables={
            "format_instructions": pydantic_parser.get_format_instructions()
        },
    )

    chain = prompt | model | pydantic_parser

    response = chain.invoke({"question": question})

    print("Parsed response:", response)

    return {"messages": [response.Topic]}


# it will route
def router(state: AgentState):
    print("-> ROUTER ->")

    last_message_from_supervisor = state["messages"][-1]
    print("last_message_from_supervisor is:", last_message_from_supervisor)

    normalized = last_message_from_supervisor.strip().lower()

    if "usa" in normalized:
        return "RAG Call"
    elif "from internet" in normalized:
        return "WEBCRAWL Call"
    else:
        return "LLM Call"


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def calling_rag(state: AgentState):
    print("-> RAG Call ->")

    # first message in agent state is actual question asked by user. next messages are outputs generated by nodes get appended
    question = state["messages"][0]

    prompt = PromptTemplate(
        template="""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""",
        input_variables=["context", "question"],
    )

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )
    result = rag_chain.invoke(question)
    return {"messages": [result]}


# LLM Function
def call_LLM(state: AgentState):
    print("-> LLM Call ->")
    question = state["messages"][0]

    # Normal LLM call
    complete_query = (
        "Anwer the follow question with you knowledge of the real world. Following is the user question: "
        + question
    )
    response = model.invoke(complete_query)
    return {"messages": [response.content]}


# LLM Function
def call_webcrawler(state: AgentState):
    print("-> webcrawler Call ->")
    question = state["messages"][0]

    # Tool 1: Search tool using DuckDuckGo
    search = DuckDuckGoSearchRun()
    # Wrap fetcher as a LangChain Tool
    fetch_tool = Tool(
        name="WebPageContentFetcher",
        func=fetch_url_text,
        description="Use this tool to fetch the raw text content of a web page when given a URL.",
    )

    # LLM
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    # Agent with both tools
    agent = initialize_agent(
        tools=[
            Tool(
                name="DuckDuckGoSearch",
                func=search.run,
                description="Use this tool to search the internet for current and recent information.",
            ),
            fetch_tool,
        ],
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=False,
    )

    # Take user input and run the agent
    if __name__ == "__main__":
        response = agent.invoke(question)
        print("\n--- Response ---\n")
        print(response)
        return {"messages": [response['output']]}


# Tool 2: Web page content fetcher
def fetch_url_text(url: str) -> str:
    try:
        print(f"Fetching content from: {url}")
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style"]):
            tag.decompose()
        return soup.get_text(separator=" ", strip=True)[:3000]  # limit to 3k chars
    except Exception as e:
        return f"Error fetching URL: {str(e)}"


# validation node
def validate_response(state: AgentState):
    print("-> VALIDATION ->")
    answer = state["messages"][-1]
    # Simple validation logic â€” customize this
    if "I don't know" in answer:
        print("Validation Failed")
        return {"validation_result": "fail", "messages": state["messages"]}
    else:
        print("Validation Passed")
        return {"validation_result": "pass", "messages": state["messages"]}


workflow = StateGraph(AgentState)

# add nodes
workflow.add_node("Supervisor", supervisor)
workflow.add_node("RAG", calling_rag)
workflow.add_node("LLM", call_LLM)
workflow.add_node("WEBCRAWL", call_webcrawler)
workflow.add_node("VALIDATE", validate_response)

workflow.add_edge("RAG", "VALIDATE")
workflow.add_edge("LLM", "VALIDATE")
workflow.add_edge("WEBCRAWL", "VALIDATE")

# add entry
workflow.set_entry_point("Supervisor")

# add conditional edge 1 at supervisor
workflow.add_conditional_edges(
    "Supervisor",
    router,
    {
        "RAG Call": "RAG",
        "LLM Call": "LLM",
        "WEBCRAWL Call": "WEBCRAWL",
    },
)


def validation_router(state: AgentState):
    return state.get("validation_result")


workflow.add_conditional_edges(
    "VALIDATE",
    validation_router,
    {
        "pass": "FinalOutput",
        "fail": "Supervisor",
    },
)


# Final output node
def final_output(state: AgentState):
    print("-> FINAL OUTPUT ->")
    return {"messages": state["messages"]}

workflow.add_node("FinalOutput", final_output)
workflow.add_edge("FinalOutput", END)


# compile langgraph
app = workflow.compile()

state = {"messages": ["How to earn money using ETF? From Internet"]}

result = app.invoke(state)

print(result["messages"][-1])
